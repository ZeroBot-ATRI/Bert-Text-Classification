#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†è’¸é¦æµ‹è¯•è„šæœ¬
éªŒè¯å®Œæ•´çš„çŸ¥è¯†è’¸é¦æµç¨‹
"""

import torch
import os
from transformers import BertTokenizer

from data_loader import DataLoader_Manager
from model import BertClassifier, ModelConfig, ModelManager
from lstm_student_model import create_lstm_student_model, LSTMModelConfig, count_lstm_parameters
from knowledge_distillation import DistillationTrainer, create_distillation_config

def test_distillation_pipeline():
    """æµ‹è¯•çŸ¥è¯†è’¸é¦å®Œæ•´æµç¨‹"""
    print("=" * 60)
    print("å¼€å§‹æµ‹è¯•çŸ¥è¯†è’¸é¦æµç¨‹")
    print("=" * 60)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®å‡†å¤‡
    print("\n1. å‡†å¤‡æ•°æ®...")
    data_manager = DataLoader_Manager('data', 'pretrain/bert-base-chinese', max_length=128)
    train_dataset, val_dataset, test_dataset = data_manager.create_datasets()
    train_loader, val_loader, test_loader = data_manager.create_dataloaders(
        train_dataset, val_dataset, test_dataset, batch_size=4
    )
    
    num_classes = data_manager.get_num_classes()
    tokenizer = BertTokenizer.from_pretrained('pretrain/bert-base-chinese')
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ - ç±»åˆ«æ•°: {num_classes}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset) if val_dataset else 0}")
    
    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    print("\n2. åŠ è½½æ•™å¸ˆæ¨¡å‹...")
    teacher_config = ModelConfig()
    teacher_config['pretrain_dir'] = 'pretrain/bert-base-chinese'
    teacher_config['num_classes'] = num_classes
    
    model_manager = ModelManager(teacher_config)
    teacher_model = model_manager.create_model()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    teacher_model_path = 'checkpoints/best_model/pytorch_model.bin'
    if os.path.exists(teacher_model_path):
        state_dict = torch.load(teacher_model_path, map_location=device)
        teacher_model.load_state_dict(state_dict)
        print("æˆåŠŸåŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡")
    else:
        print("è­¦å‘Š: æ•™å¸ˆæ¨¡å‹æƒé‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    teacher_model.to(device)
    teacher_model.eval()
    
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°æ•°é‡: {teacher_params:,}")
    
    # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    print("\n3. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
    student_config = LSTMModelConfig()
    student_config['model_type'] = 'enhanced_bilstm_student'
    student_config['vocab_size'] = tokenizer.vocab_size
    student_config['embedding_dim'] = 128  # è¾ƒå°çš„åµŒå…¥ç»´åº¦ç”¨äºæµ‹è¯•
    student_config['hidden_dim'] = 256    # è¾ƒå°çš„éšè—ç»´åº¦ç”¨äºæµ‹è¯•
    student_config['num_layers'] = 2
    student_config['num_classes'] = num_classes
    
    student_model = create_lstm_student_model(student_config, tokenizer)
    student_model.to(device)
    
    student_params = sum(p.numel() for p in student_model.parameters())
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°æ•°é‡: {student_params:,}")
    print(f"å‹ç¼©æ¯”: {teacher_params / student_params:.1f}x")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n4. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    
    # è·å–ä¸€ä¸ªå°æ‰¹æ¬¡æ•°æ®
    batch = next(iter(train_loader))
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
    with torch.no_grad():
        teacher_outputs = teacher_model(input_ids, attention_mask)
        teacher_logits = teacher_outputs['logits']
        teacher_features = teacher_outputs.get('pooler_output', None)
        if teacher_features is None:
            # å¦‚æœæ²¡æœ‰pooler_outputï¼Œä½¿ç”¨last_hidden_stateçš„[CLS]ä½ç½®
            teacher_features = teacher_model.bert(input_ids, attention_mask).last_hidden_state[:, 0, :]
    
    print(f"æ•™å¸ˆæ¨¡å‹è¾“å‡ºå½¢çŠ¶: {teacher_logits.shape}")
    print(f"æ•™å¸ˆæ¨¡å‹ç‰¹å¾å½¢çŠ¶: {teacher_features.shape}")
    
    # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
    student_outputs = student_model(input_ids, attention_mask, return_features=True)
    student_logits = student_outputs['logits']
    student_features = student_outputs['features']
    
    print(f"å­¦ç”Ÿæ¨¡å‹è¾“å‡ºå½¢çŠ¶: {student_logits.shape}")
    print(f"å­¦ç”Ÿæ¨¡å‹ç‰¹å¾å½¢çŠ¶: {student_features.shape}")
    print(f"å­¦ç”Ÿæ¨¡å‹åŸå§‹ç‰¹å¾å½¢çŠ¶: {student_outputs['lstm_features'].shape}")
    
    # åˆ›å»ºè’¸é¦è®­ç»ƒå™¨
    print("\n5. åˆ›å»ºè’¸é¦è®­ç»ƒå™¨...")
    distill_config = create_distillation_config()
    
    # æ ¹æ®å®é™…çš„ç‰¹å¾ç»´åº¦è®¾ç½®é…ç½®
    actual_student_dim = student_outputs['lstm_features'].shape[-1]  # åŸå§‹ç‰¹å¾ç»´åº¦
    projected_student_dim = student_outputs['features'].shape[-1]  # æŠ•å½±åç‰¹å¾ç»´åº¦
    print(f"å®é™…å­¦ç”Ÿæ¨¡å‹ç‰¹å¾ç»´åº¦: {actual_student_dim}")
    print(f"æŠ•å½±åå­¦ç”Ÿæ¨¡å‹ç‰¹å¾ç»´åº¦: {projected_student_dim}")
    
    distill_config.update({
        'temperature': 3.0,
        'alpha': 0.8,
        'feature_loss_weight': 0.1,
        'student_feature_dim': projected_student_dim,  # ä½¿ç”¨æŠ•å½±åçš„ç»´åº¦
        'teacher_feature_dim': 768,
        'learning_rate': 5e-4,
        'warmup_steps': 100
    })
    
    trainer = DistillationTrainer(teacher_model, student_model, distill_config)
    
    # çŸ­æ—¶é—´è®­ç»ƒæµ‹è¯•
    print("\n6. å¼€å§‹çŸ­æ—¶é—´è®­ç»ƒæµ‹è¯• (1ä¸ªepoch)...")
    
    # åˆ›å»ºå°çš„æ•°æ®åŠ è½½å™¨ç”¨äºæµ‹è¯•
    small_train_dataset = torch.utils.data.Subset(train_dataset, range(min(20, len(train_dataset))))
    small_train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=4, shuffle=True)
    
    small_val_dataset = torch.utils.data.Subset(val_dataset, range(min(10, len(val_dataset)))) if val_dataset else None
    small_val_loader = torch.utils.data.DataLoader(small_val_dataset, batch_size=4, shuffle=False) if small_val_dataset else None
    
    try:
        history = trainer.train(
            train_loader=small_train_loader,
            val_loader=small_val_loader,
            device=device,
            num_epochs=1,
            save_dir='test_distilled_models'
        )
        
        print("\nè®­ç»ƒæµ‹è¯•å®Œæˆ!")
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {history[-1]['train']['loss']:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history[-1]['train']['accuracy']:.4f}")
        
        if history[-1]['val']:
            print(f"éªŒè¯å‡†ç¡®ç‡: {history[-1]['val']['accuracy']:.4f}")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\n7. æµ‹è¯•å­¦ç”Ÿæ¨¡å‹æ¨ç†...")
    
    # æµ‹è¯•å­¦ç”Ÿæ¨¡å‹ç‹¬ç«‹æ¨ç†
    student_model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in small_train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = student_model(input_ids, attention_mask)
            predictions = torch.argmax(outputs['logits'], dim=1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    
    accuracy = correct / total
    print(f"å­¦ç”Ÿæ¨¡å‹åœ¨å°æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡: {accuracy:.4f}")
    
    print("\n" + "=" * 60)
    print("çŸ¥è¯†è’¸é¦æµç¨‹æµ‹è¯•å®Œæˆ!")
    print("âœ… æ‰€æœ‰ç»„ä»¶å·¥ä½œæ­£å¸¸")
    print("=" * 60)
    
    return True

if __name__ == "__main__":
    success = test_distillation_pipeline()
    if success:
        print("\nğŸ‰ çŸ¥è¯†è’¸é¦ç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")