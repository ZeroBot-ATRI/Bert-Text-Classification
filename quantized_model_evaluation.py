#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é‡åŒ–æ¨¡å‹è¯„ä¼°è„šæœ¬
æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€æ¨ç†é€Ÿåº¦ã€å†…å­˜å ç”¨ç­‰æŒ‡æ ‡
"""

import os
import time
import json
import logging
import psutil
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

from evaluate import ModelEvaluator
from model import ModelManager, ModelConfig
from model_optimization import ModelOptimizer
from true_quantization import TrueModelOptimizer, TrueQuantizedLinear
from data_loader import DataLoader_Manager

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuantizedModelEvaluator:
    """é‡åŒ–æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, original_model_path: str, data_dir: str = "data"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
            data_dir: æ•°æ®ç›®å½•
        """
        self.original_model_path = original_model_path
        self.data_dir = data_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        config = ModelConfig()
        self.model_manager = ModelManager(config)
        self.original_model = self.model_manager.load_model(original_model_path)
        
        # åŠ è½½æ•°æ®ç®¡ç†å™¨
        self.data_manager = DataLoader_Manager(data_dir, config['pretrain_dir'])
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.evaluation_results = {}
        
        logger.info("é‡åŒ–æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_model_size_info(self, model: nn.Module, model_name: str) -> Dict:
        """è·å–æ¨¡å‹å¤§å°ä¿¡æ¯"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        total_size = param_size + buffer_size
        
        # ç»Ÿè®¡é‡åŒ–å±‚ä¿¡æ¯
        quantized_layers = 0
        total_linear_layers = 0
        
        for module in model.modules():
            if isinstance(module, nn.Linear):
                total_linear_layers += 1
            elif isinstance(module, TrueQuantizedLinear):
                quantized_layers += 1
                total_linear_layers += 1
        
        return {
            'model_name': model_name,
            'total_size_mb': total_size / 1024 / 1024,
            'param_size_mb': param_size / 1024 / 1024,
            'buffer_size_mb': buffer_size / 1024 / 1024,
            'total_params': sum(p.numel() for p in model.parameters()),
            'quantized_layers': quantized_layers,
            'total_linear_layers': total_linear_layers,
            'quantization_ratio': quantized_layers / total_linear_layers if total_linear_layers > 0 else 0
        }
    
    def measure_inference_speed(self, model: nn.Module, test_texts: List[str], 
                              num_runs: int = 10) -> Dict:
        """æµ‹é‡æ¨ç†é€Ÿåº¦"""
        model.eval()
        model.to(self.device)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_sample = test_texts[:min(100, len(test_texts))]  # ä½¿ç”¨å‰100ä¸ªæ ·æœ¬
        
        # é¢„çƒ­
        with torch.no_grad():
            for text in test_sample[:5]:
                try:
                    # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹è¾“å…¥æ ¼å¼è°ƒæ•´
                    inputs = self.data_manager.tokenizer.encode_plus(
                        text,
                        add_special_tokens=True,
                        max_length=512,
                        padding='max_length',
                        truncation=True,
                        return_tensors='pt'
                    )
                    
                    input_ids = inputs['input_ids'].to(self.device)
                    attention_mask = inputs['attention_mask'].to(self.device)
                    
                    _ = model(input_ids, attention_mask)
                except Exception as e:
                    logger.warning(f"é¢„çƒ­æ—¶å‡ºé”™: {e}")
                    break
        
        # æ­£å¼æµ‹è¯•
        times = []
        
        for run in range(num_runs):
            start_time = time.time()
            
            with torch.no_grad():
                for text in test_sample:
                    try:
                        inputs = self.data_manager.tokenizer.encode_plus(
                            text,
                            add_special_tokens=True,
                            max_length=512,
                            padding='max_length',
                            truncation=True,
                            return_tensors='pt'
                        )
                        
                        input_ids = inputs['input_ids'].to(self.device)
                        attention_mask = inputs['attention_mask'].to(self.device)
                        
                        _ = model(input_ids, attention_mask)
                    except Exception as e:
                        logger.warning(f"æ¨ç†æ—¶å‡ºé”™: {e}")
                        continue
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        
        return {
            'avg_time_per_batch': avg_time,
            'std_time_per_batch': std_time,
            'avg_time_per_sample': avg_time / len(test_sample),
            'throughput_samples_per_sec': len(test_sample) / avg_time,
            'num_samples': len(test_sample),
            'num_runs': num_runs
        }
    
    def measure_memory_usage(self, model: nn.Module) -> Dict:
        """æµ‹é‡å†…å­˜å ç”¨"""
        # GPUå†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gpu_memory_before = torch.cuda.memory_allocated()
            
            model.to(self.device)
            
            gpu_memory_after = torch.cuda.memory_allocated()
            gpu_memory_used = (gpu_memory_after - gpu_memory_before) / 1024 / 1024
        else:
            gpu_memory_used = 0
        
        # CPUå†…å­˜
        process = psutil.Process()
        cpu_memory_info = process.memory_info()
        
        return {
            'gpu_memory_mb': gpu_memory_used,
            'cpu_memory_mb': cpu_memory_info.rss / 1024 / 1024,
            'cpu_memory_percent': process.memory_percent()
        }
    
    def evaluate_model_accuracy(self, model: nn.Module, model_name: str, 
                              dataset_type: str = 'test') -> Dict:
        """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
        logger.info(f"è¯„ä¼° {model_name} åœ¨ {dataset_type} æ•°æ®é›†ä¸Šçš„ç²¾åº¦...")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        if dataset_type == 'test':
            file_path = os.path.join(self.data_dir, 'test.txt')
        else:
            file_path = os.path.join(self.data_dir, 'dev.txt')
        
        texts, true_labels = self.data_manager.load_data_from_file(file_path)
        
        # è¿›è¡Œé¢„æµ‹
        model.eval()
        model.to(self.device)
        
        predictions = []
        correct = 0
        total = 0
        
        with torch.no_grad():
            for i, (text, true_label) in enumerate(tqdm(zip(texts, true_labels), 
                                                       desc=f"è¯„ä¼°{model_name}")):
                try:
                    inputs = self.data_manager.tokenizer.encode_plus(
                        text,
                        add_special_tokens=True,
                        max_length=512,
                        padding='max_length',
                        truncation=True,
                        return_tensors='pt'
                    )
                    
                    input_ids = inputs['input_ids'].to(self.device)
                    attention_mask = inputs['attention_mask'].to(self.device)
                    
                    outputs = model(input_ids, attention_mask)
                    
                    if isinstance(outputs, dict):
                        logits = outputs['logits']
                    else:
                        logits = outputs
                    
                    predicted_label = torch.argmax(logits, dim=1).item()
                    predictions.append(predicted_label)
                    
                    if predicted_label == true_label:
                        correct += 1
                    total += 1
                    
                except Exception as e:
                    logger.warning(f"é¢„æµ‹ç¬¬ {i} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                    predictions.append(0)  # é»˜è®¤é¢„æµ‹
                    total += 1
        
        accuracy = correct / total if total > 0 else 0
        
        return {
            'model_name': model_name,
            'dataset_type': dataset_type,
            'accuracy': accuracy,
            'correct': correct,
            'total': total,
            'predictions': predictions,
            'true_labels': true_labels
        }
    
    def compare_models(self, quantized_model_paths: List[str]) -> Dict:
        """æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹"""
        logger.info("å¼€å§‹æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹...")
        
        # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆç”¨äºé€Ÿåº¦æµ‹è¯•ï¼‰
        test_file = os.path.join(self.data_dir, 'test.txt')
        test_texts, test_labels = self.data_manager.load_data_from_file(test_file)
        test_texts = test_texts[:50]  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
        
        comparison_results = {
            'models': {},
            'summary': {}
        }
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        logger.info("è¯„ä¼°åŸå§‹æ¨¡å‹...")
        
        original_size_info = self.get_model_size_info(self.original_model, "åŸå§‹æ¨¡å‹")
        original_speed_info = self.measure_inference_speed(self.original_model, test_texts)
        original_memory_info = self.measure_memory_usage(self.original_model)
        original_accuracy_info = self.evaluate_model_accuracy(self.original_model, "åŸå§‹æ¨¡å‹")
        
        comparison_results['models']['original'] = {
            'size': original_size_info,
            'speed': original_speed_info,
            'memory': original_memory_info,
            'accuracy': original_accuracy_info
        }
        
        # è¯„ä¼°é‡åŒ–æ¨¡å‹
        for i, quantized_path in enumerate(quantized_model_paths):
            model_name = f"é‡åŒ–æ¨¡å‹_{i+1}"
            
            if not os.path.exists(quantized_path):
                logger.warning(f"é‡åŒ–æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {quantized_path}")
                continue
            
            logger.info(f"è¯„ä¼° {model_name}...")
            
            try:
                # åŠ è½½é‡åŒ–æ¨¡å‹
                optimizer = ModelOptimizer(self.original_model_path)
                optimizer.load_optimized_model(quantized_path)
                quantized_model = optimizer.optimized_model
                
                if quantized_model is None:
                    logger.warning(f"åŠ è½½é‡åŒ–æ¨¡å‹å¤±è´¥: {quantized_path}")
                    continue
                
                # è¯„ä¼°å„é¡¹æŒ‡æ ‡
                size_info = self.get_model_size_info(quantized_model, model_name)
                speed_info = self.measure_inference_speed(quantized_model, test_texts)
                memory_info = self.measure_memory_usage(quantized_model)
                accuracy_info = self.evaluate_model_accuracy(quantized_model, model_name)
                
                comparison_results['models'][f'quantized_{i+1}'] = {
                    'path': quantized_path,
                    'size': size_info,
                    'speed': speed_info,
                    'memory': memory_info,
                    'accuracy': accuracy_info
                }
                
            except Exception as e:
                logger.error(f"è¯„ä¼°é‡åŒ–æ¨¡å‹ {quantized_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # è®¡ç®—æ”¹å–„æ¯”ä¾‹
        self._calculate_improvements(comparison_results)
        
        return comparison_results
    
    def _calculate_improvements(self, comparison_results: Dict):
        """è®¡ç®—æ”¹å–„æ¯”ä¾‹"""
        if 'original' not in comparison_results['models']:
            return
        
        original = comparison_results['models']['original']
        
        comparison_results['summary'] = {
            'original_model': {
                'size_mb': original['size']['total_size_mb'],
                'accuracy': original['accuracy']['accuracy'],
                'inference_speed': original['speed']['throughput_samples_per_sec']
            },
            'quantized_models': {}
        }
        
        for key, model_data in comparison_results['models'].items():
            if key == 'original':
                continue
            
            quantized = model_data
            
            # è®¡ç®—å‹ç¼©æ¯”
            size_reduction = (1 - quantized['size']['total_size_mb'] / 
                            original['size']['total_size_mb']) * 100
            
            # è®¡ç®—ç²¾åº¦æŸå¤±
            accuracy_loss = (original['accuracy']['accuracy'] - 
                           quantized['accuracy']['accuracy']) * 100
            
            # è®¡ç®—é€Ÿåº¦æå‡
            speed_improvement = (quantized['speed']['throughput_samples_per_sec'] / 
                               original['speed']['throughput_samples_per_sec'] - 1) * 100
            
            comparison_results['summary']['quantized_models'][key] = {
                'size_mb': quantized['size']['total_size_mb'],
                'size_reduction_percent': size_reduction,
                'accuracy': quantized['accuracy']['accuracy'],
                'accuracy_loss_percent': accuracy_loss,
                'inference_speed': quantized['speed']['throughput_samples_per_sec'],
                'speed_improvement_percent': speed_improvement
            }
    
    def save_evaluation_results(self, results: Dict, save_dir: str = "evaluation_results"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(save_dir, 'quantized_model_evaluation.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = os.path.join(save_dir, 'quantized_model_report.txt')
        self._generate_report(results, report_file)
        
        # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
        self._plot_comparison_charts(results, save_dir)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    
    def _generate_report(self, results: Dict, report_file: str):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("é‡åŒ–æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            # æ‘˜è¦
            if 'summary' in results:
                f.write("ğŸ“Š è¯„ä¼°æ‘˜è¦\n")
                f.write("-" * 30 + "\n")
                
                summary = results['summary']
                
                # åŸå§‹æ¨¡å‹ä¿¡æ¯
                if 'original_model' in summary:
                    orig = summary['original_model']
                    f.write(f"åŸå§‹æ¨¡å‹:\n")
                    f.write(f"  æ¨¡å‹å¤§å°: {orig['size_mb']:.2f} MB\n")
                    f.write(f"  å‡†ç¡®ç‡: {orig['accuracy']:.4f}\n")
                    f.write(f"  æ¨ç†é€Ÿåº¦: {orig['inference_speed']:.2f} æ ·æœ¬/ç§’\n\n")
                
                # é‡åŒ–æ¨¡å‹å¯¹æ¯”
                if 'quantized_models' in summary:
                    for model_name, model_data in summary['quantized_models'].items():
                        f.write(f"{model_name}:\n")
                        f.write(f"  æ¨¡å‹å¤§å°: {model_data['size_mb']:.2f} MB "
                               f"(å‡å°‘ {model_data['size_reduction_percent']:.1f}%)\n")
                        f.write(f"  å‡†ç¡®ç‡: {model_data['accuracy']:.4f} "
                               f"(æŸå¤± {model_data['accuracy_loss_percent']:.2f}%)\n")
                        f.write(f"  æ¨ç†é€Ÿåº¦: {model_data['inference_speed']:.2f} æ ·æœ¬/ç§’ "
                               f"(æå‡ {model_data['speed_improvement_percent']:.1f}%)\n\n")
            
            # è¯¦ç»†ç»“æœ
            f.write("\nğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœ\n")
            f.write("-" * 30 + "\n")
            
            for model_key, model_data in results['models'].items():
                f.write(f"\n{model_data['size']['model_name']}:\n")
                f.write(f"  æ¨¡å‹å¤§å°: {model_data['size']['total_size_mb']:.2f} MB\n")
                f.write(f"  å‚æ•°æ•°é‡: {model_data['size']['total_params']:,}\n")
                f.write(f"  é‡åŒ–å±‚æ•°: {model_data['size']['quantized_layers']}/{model_data['size']['total_linear_layers']}\n")
                f.write(f"  å‡†ç¡®ç‡: {model_data['accuracy']['accuracy']:.4f}\n")
                f.write(f"  æ¨ç†é€Ÿåº¦: {model_data['speed']['throughput_samples_per_sec']:.2f} æ ·æœ¬/ç§’\n")
                f.write(f"  GPUå†…å­˜: {model_data['memory']['gpu_memory_mb']:.2f} MB\n")
    
    def _plot_comparison_charts(self, results: Dict, save_dir: str):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
        if 'summary' not in results or 'quantized_models' not in results['summary']:
            return
        
        summary = results['summary']
        
        # å‡†å¤‡æ•°æ®
        model_names = ['åŸå§‹æ¨¡å‹']
        sizes = [summary['original_model']['size_mb']]
        accuracies = [summary['original_model']['accuracy']]
        speeds = [summary['original_model']['inference_speed']]
        
        for model_name, model_data in summary['quantized_models'].items():
            model_names.append(model_name.replace('quantized_', 'é‡åŒ–æ¨¡å‹'))
            sizes.append(model_data['size_mb'])
            accuracies.append(model_data['accuracy'])
            speeds.append(model_data['inference_speed'])
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ¨¡å‹å¤§å°å¯¹æ¯”
        bars1 = ax1.bar(model_names, sizes, color=['blue', 'red', 'green', 'orange'][:len(sizes)])
        ax1.set_title('æ¨¡å‹å¤§å°å¯¹æ¯”')
        ax1.set_ylabel('å¤§å° (MB)')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, size in zip(bars1, sizes):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{size:.1f}MB', ha='center', va='bottom')
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        bars2 = ax2.bar(model_names, accuracies, color=['blue', 'red', 'green', 'orange'][:len(accuracies)])
        ax2.set_title('å‡†ç¡®ç‡å¯¹æ¯”')
        ax2.set_ylabel('å‡†ç¡®ç‡')
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        
        for bar, acc in zip(bars2, accuracies):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom')
        
        # æ¨ç†é€Ÿåº¦å¯¹æ¯”
        bars3 = ax3.bar(model_names, speeds, color=['blue', 'red', 'green', 'orange'][:len(speeds)])
        ax3.set_title('æ¨ç†é€Ÿåº¦å¯¹æ¯”')
        ax3.set_ylabel('æ ·æœ¬/ç§’')
        ax3.tick_params(axis='x', rotation=45)
        
        for bar, speed in zip(bars3, speeds):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{speed:.1f}', ha='center', va='bottom')
        
        # å‹ç¼©æ•ˆæœå›¾
        if len(sizes) > 1:
            reduction_rates = [(1 - size/sizes[0]) * 100 for size in sizes[1:]]
            bars4 = ax4.bar(model_names[1:], reduction_rates, color=['red', 'green', 'orange'][:len(reduction_rates)])
            ax4.set_title('æ¨¡å‹å‹ç¼©æ•ˆæœ')
            ax4.set_ylabel('å‹ç¼©ç‡ (%)')
            ax4.tick_params(axis='x', rotation=45)
            
            for bar, rate in zip(bars4, reduction_rates):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{rate:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'quantized_model_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é‡åŒ–æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model_path', type=str, default='checkpoints/best_model',
                       help='åŸå§‹æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='data',
                       help='æ•°æ®ç›®å½•')
    parser.add_argument('--quantized_models', type=str, nargs='+',
                       default=['optimized_models/nf4_quantized', 
                               'optimized_models/combined_pruned_0.2_nf4'],
                       help='é‡åŒ–æ¨¡å‹è·¯å¾„åˆ—è¡¨')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = QuantizedModelEvaluator(args.model_path, args.data_dir)
    
    # è¿›è¡Œæ¯”è¾ƒè¯„ä¼°
    results = evaluator.compare_models(args.quantized_models)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_results(results, args.output_dir)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š é‡åŒ–æ¨¡å‹è¯„ä¼°å®Œæˆ")
    print("="*60)
    
    if 'summary' in results:
        summary = results['summary']
        
        if 'original_model' in summary:
            orig = summary['original_model']
            print(f"\nåŸå§‹æ¨¡å‹:")
            print(f"  å¤§å°: {orig['size_mb']:.2f} MB")
            print(f"  å‡†ç¡®ç‡: {orig['accuracy']:.4f}")
            print(f"  é€Ÿåº¦: {orig['inference_speed']:.2f} æ ·æœ¬/ç§’")
        
        if 'quantized_models' in summary:
            for model_name, model_data in summary['quantized_models'].items():
                print(f"\n{model_name}:")
                print(f"  å¤§å°: {model_data['size_mb']:.2f} MB (å‡å°‘ {model_data['size_reduction_percent']:.1f}%)")
                print(f"  å‡†ç¡®ç‡: {model_data['accuracy']:.4f} (æŸå¤± {model_data['accuracy_loss_percent']:.2f}%)")
                print(f"  é€Ÿåº¦: {model_data['inference_speed']:.2f} æ ·æœ¬/ç§’ (æå‡ {model_data['speed_improvement_percent']:.1f}%)")
    
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")

if __name__ == "__main__":
    main()